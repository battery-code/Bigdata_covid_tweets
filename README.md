# BigDataCovidTweets
## Objective
<ul>
        <li> Utilize PySpark, Parquet format, NLP Techniques and Google Cloud Platform to analyse Big Data; 100 Million tweets related to Covid </li>
        <li> Identify top Tweeters and personas </li>
        <li> Extract meaning from analysis of volume, location and timeline </li>
        <li> Identify contexts for patterns and contents in the data </li>
</ul>

## Data Profile
<ul>
        <li> 100Million Tweets, 617 Gigabytes</li>
        <li> Data Dates: Oct 15,2021 to Jan 25,2022</li>
        <li> Form: Tabular data of Tweets and related Meta data like timestamp, location and user identification</li>
        <li> File format: JSON</li>
</ul>

## Tech Stack
<ul>
        <li> PySpark in JupyterLabs and GCP platform</li>
        <li> Parquet file format partioned by date for efficient and speedy analysis</li>
</ul>

## Techniques
<ul>
        <li> NLP; Bag-of-Words (BoW) Analysis, NGram Analysis, RegEx</li>
        <li> NLP Packages: Gensim, NLTK, SPacy </li>
</ul>
