{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Tweets from msca-bdp-tweets bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PySpark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark.sql.functions as sql_fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'gs://msca-bdp-tweets/final_project/'\n",
    "# file = '*.json'\n",
    "# path = directory + file\n",
    "path = directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 22:27:52 WARN org.apache.spark.sql.execution.datasources.SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 538 ms, total: 2.54 s\n",
      "Wall time: 10min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 22:38:35 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tweets_df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "sample_df = tweets_df.select(col(\"id\"),col(\"id_str\"),col(\"user\"),col(\"place\"),col(\"text\"),col(\"retweeted_status\"), col(\"extended_tweet\"), \\\n",
    "                             col(\"entities\"),col(\"created_at\"), col(\"coordinates\"),col(\"favorite_count\"), \\\n",
    "                             col(\"quote_count\"),col(\"retweet_count\"),col(\"reply_count\"),col(\"is_quote_status\"), \\\n",
    "                             col(\"retweeted\"),col(\"lang\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_fun\n",
    "sample_df = sample_df.withColumn('text',sql_fun.lower(sample_df.text)).withColumn('retweeted_status_text',sql_fun.lower(sample_df.retweeted_status.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>place</th><th>text</th><th>retweeted_status</th><th>extended_tweet</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>retweeted_status_text</th><th>created_at_year</th><th>created_at_mon</th><th>created_at_day</th></tr>\n",
       "<tr><td>1469800322834976770</td><td>1469800322834976770</td><td>{false, Wed Aug 2...</td><td>null</td><td>@vlg890 @sandiesw...</td><td>null</td><td>{[39, 190], {[], ...</td><td>{[], null, [], [{...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>null</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323019468801</td><td>1469800323019468801</td><td>{false, Tue May 1...</td><td>null</td><td>no it’s not that....</td><td>null</td><td>{[0, 166], {[], n...</td><td>{[], null, [], [{...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>true</td><td>false</td><td>en</td><td>null</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323095146498</td><td>1469800323095146498</td><td>{false, Thu Feb 0...</td><td>null</td><td>rt @ollysmithtrav...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>we will need @ukl...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323254472709</td><td>1469800323254472709</td><td>{false, Fri Nov 0...</td><td>null</td><td>@iluvchibiis covi...</td><td>null</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>null</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323564851204</td><td>1469800323564851204</td><td>{false, Thu Sep 2...</td><td>null</td><td>rt @comradevero: ...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>if you’re triple ...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------------+---------------+--------------+--------------+\n",
       "|                 id|             id_str|                user|place|                text|    retweeted_status|      extended_tweet|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|retweeted_status_text|created_at_year|created_at_mon|created_at_day|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------------+---------------+--------------+--------------+\n",
       "|1465044848516517900|1465044848516517900|{false, Sun Oct 2...| null|rt @aginnt: from ...|{null, null, Sun ...|                null|{[], [{{null, nul...|Sun Nov 28 19:48:...|       null|             0|          0|            0|          0|          false|     true|  en| from casablanca, ...|           2021|            11|            28|\n",
       "|1465044849011343377|1465044849011343377|{false, Sat Jan 3...| null|rt @waitingonbide...|{null, null, Sun ...|                null|{[], null, [], []...|Sun Nov 28 19:48:...|       null|             0|          0|            0|          0|          false|     true|  en| it couldn’t be cl...|           2021|            11|            28|\n",
       "|1465044851003510784|1465044851003510784|{false, Mon May 1...| null|@cbotheeggman @la...|                null|{[29, 263], {[], ...|{[], null, [], [{...|Sun Nov 28 19:48:...|       null|             0|          0|            0|          0|          false|    false|  en|                 null|           2021|            11|            28|\n",
       "|1465044851154690056|1465044851154690056|{false, Wed Aug 1...| null|rt @hoaxtrump9993...|{null, null, Sun ...|                null|{[{[61, 72], vent...|Sun Nov 28 19:48:...|       null|             0|          0|            0|          0|           true|     true|  en| why risk your chi...|           2021|            11|            28|\n",
       "|1465044851691458560|1465044851691458560|{false, Fri Mar 0...| null|rt @rwmalonemd: a...|{null, null, Sun ...|                null|{[], null, [], [{...|Sun Nov 28 19:48:...|       null|             0|          0|            0|          0|          false|     true|  en| a president betra...|           2021|            11|            28|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------------+---------------+--------------+--------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|retweeted_status|\n",
      "+----------------+\n",
      "|        30331729|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "col_list = ['retweeted_status']\n",
    "sample_df.select([count(when(col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61852096"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df = sample_df.filter(sample_df.text.contains('covid')|sample_df.retweeted_status_text.contains('covid'))\n",
    "covid_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_df.withColumn('retweeted', when(covid_df.retweeted_status.isNull(), False).otherwise(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date extraction using regexp\n",
    "covid_df = covid_df.withColumn(\"created_at_year\", regexp_extract(\"created_at\",\"20\\d\\d$\",0))\n",
    "covid_df = covid_df.withColumn(\"created_at_mon\", month(to_date(trim(regexp_extract(\"created_at\",\"\\s\\w+\",0)),\"MMM\")))\n",
    "covid_df = covid_df.withColumn(\"created_at_day\", trim(regexp_extract(\"created_at\",\"\\s\\d\\d\\s\",0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_df.withColumn('user_name',col('user.name')) \\\n",
    "                .withColumn('user_scrname',col('user.screen_name')) \\\n",
    "                .withColumn('user_desc',col('user.description')) \\\n",
    "                .withColumn('user_id',col('user.id')) \\\n",
    "                .withColumn('user_loc',col('user.location')) \\\n",
    "                .withColumn('user_url',col('user.url')) \\\n",
    "                .withColumn('user_followers',col('user.followers_count')) \\\n",
    "                .withColumn('user_following',col('user.following')) \\\n",
    "                .withColumn('user_friends_count',col('user.friends_count')) \\\n",
    "                .withColumn('user-verified',col('user.verified')) \\\n",
    "                .withColumn('user_dpimage',col('user.default_profile_image'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid_df = covid_df.drop('user.name', 'user.scrname', 'user.desc', 'user.id', 'user.loc', 'user.url', 'user.followers', 'user.following', 'user.friends_count', 'user.verified', 'user.dpimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'id_str',\n",
       " 'user',\n",
       " 'place',\n",
       " 'text',\n",
       " 'extended_tweet',\n",
       " 'entities',\n",
       " 'created_at',\n",
       " 'coordinates',\n",
       " 'favorite_count',\n",
       " 'quote_count',\n",
       " 'retweet_count',\n",
       " 'reply_count',\n",
       " 'is_quote_status',\n",
       " 'retweeted',\n",
       " 'lang',\n",
       " 'created_at_year',\n",
       " 'created_at_mon',\n",
       " 'created_at_day']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>place</th><th>text</th><th>retweeted_status</th><th>extended_tweet</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>retweeted_status_text</th><th>created_at_year</th><th>created_at_mon</th><th>created_at_day</th></tr>\n",
       "<tr><td>1469800322834976770</td><td>1469800322834976770</td><td>{false, Wed Aug 2...</td><td>null</td><td>@vlg890 @sandiesw...</td><td>null</td><td>{[39, 190], {[], ...</td><td>{[], null, [], [{...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>null</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323019468801</td><td>1469800323019468801</td><td>{false, Tue May 1...</td><td>null</td><td>no it’s not that....</td><td>null</td><td>{[0, 166], {[], n...</td><td>{[], null, [], [{...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>true</td><td>false</td><td>en</td><td>null</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323095146498</td><td>1469800323095146498</td><td>{false, Thu Feb 0...</td><td>null</td><td>rt @ollysmithtrav...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>we will need @ukl...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323254472709</td><td>1469800323254472709</td><td>{false, Fri Nov 0...</td><td>null</td><td>@iluvchibiis covi...</td><td>null</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>null</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323564851204</td><td>1469800323564851204</td><td>{false, Thu Sep 2...</td><td>null</td><td>rt @comradevero: ...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>if you’re triple ...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323573239819</td><td>1469800323573239819</td><td>{false, Thu Jul 1...</td><td>null</td><td>rt @pippacrerar: ...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>exclusive: boris ...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323787083776</td><td>1469800323787083776</td><td>{false, Wed Aug 2...</td><td>null</td><td>rt @mirrorpolitic...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], [{...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>boris johnson pic...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323921465345</td><td>1469800323921465345</td><td>{false, Wed Mar 0...</td><td>null</td><td>rt @politicsforal...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>🚨🚨🚨🚨 | breaki...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800323984285702</td><td>1469800323984285702</td><td>{false, Wed Oct 0...</td><td>null</td><td>rt @pippacrerar: ...</td><td>{null, null, Sat ...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>exclusive: boris ...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "<tr><td>1469800324823060483</td><td>1469800324823060483</td><td>{false, Fri Jul 2...</td><td>null</td><td>rt @deepbluecrypt...</td><td>{null, null, Wed ...</td><td>null</td><td>{[], [{{null, nul...</td><td>Sat Dec 11 22:44:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>true</td><td>en</td><td>are these genuine...</td><td>2021</td><td>12</td><td>11</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------------+---------------+--------------+--------------+\n",
       "|                 id|             id_str|                user|place|                text|    retweeted_status|      extended_tweet|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|retweeted_status_text|created_at_year|created_at_mon|created_at_day|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------------+---------------+--------------+--------------+\n",
       "|1458995060893097989|1458995060893097989|{false, Sun Oct 1...| null|rt @panales2011: ...|{null, null, Wed ...|                null|{[], null, [], []...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|     true|  en| from a doctor's p...|           2021|            11|            12|\n",
       "|1458995061912449025|1458995061912449025|{false, Fri Jul 1...| null|rt @mrbobodenkirk...|{null, null, Fri ...|                null|{[], [{null, null...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|     true|  en| kaboom! moderna b...|           2021|            11|            12|\n",
       "|1458995061857734658|1458995061857734658|{false, Thu Aug 0...| null|@bilel_adel @shib...|                null|{[26, 219], {[], ...|{[], null, [], [{...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|    false|  en|                 null|           2021|            11|            12|\n",
       "|1458995062231216135|1458995062231216135|{false, Sun Sep 2...| null|rt @spurnagain: a...|{null, null, Thu ...|                null|{[], null, [], []...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|     true|  en| a sad data point ...|           2021|            11|            12|\n",
       "|1458995062927331328|1458995062927331328|{false, Wed Dec 0...| null|@soundmazedj @gen...|                null|{[26, 189], {[], ...|{[], null, [], [{...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|    false|  en|                 null|           2021|            11|            12|\n",
       "|1458995063585849346|1458995063585849346|{false, Mon Feb 1...| null|rt @makismd: shoc...|{null, null, Fri ...|                null|{[], null, [], []...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|           true|     true|  en| shockingly, it is...|           2021|            11|            12|\n",
       "|1458995064164732954|1458995064164732954|{false, Sun Oct 0...| null|fucale with a shu...|                null|                null|{[], null, [], []...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|    false|  en|                 null|           2021|            11|            12|\n",
       "|1458995064361795589|1458995064361795589|{false, Fri Mar 0...| null|rt @organizingpow...|{null, null, Thu ...|                null|{[], null, [], []...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|     true|  en| cuba has vaccinat...|           2021|            11|            12|\n",
       "|1458995063439118363|1458995063439118363|{false, Tue Nov 0...| null|🔴brazilian presi...|                null|                null|{[], [{{null, nul...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|    false|  en|                 null|           2021|            11|            12|\n",
       "|1458995065901121537|1458995065901121537|{false, Sun Aug 2...| null|rt @rwmalonemd: \"...|{null, null, Fri ...|                null|{[], null, [], []...|Fri Nov 12 03:08:...|       null|             0|          0|            0|          0|          false|     true|  en| \"cecc says approv...|           2021|            11|            12|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------------+---------------+--------------+--------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# partition and parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 23:47:56 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1647641937773_0001_01_000022 on host: hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-18 23:47:56.779]Container killed on request. Exit code is 143\n",
      "[2022-03-18 23:47:56.780]Container exited with a non-zero exit code 143. \n",
      "[2022-03-18 23:47:56.784]Killed by external signal\n",
      ".\n",
      "22/03/18 23:47:56 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 22 for reason Container from a bad node: container_1647641937773_0001_01_000022 on host: hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-18 23:47:56.779]Container killed on request. Exit code is 143\n",
      "[2022-03-18 23:47:56.780]Container exited with a non-zero exit code 143. \n",
      "[2022-03-18 23:47:56.784]Killed by external signal\n",
      ".\n",
      "22/03/18 23:47:56 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 22 on hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal: Container from a bad node: container_1647641937773_0001_01_000022 on host: hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-18 23:47:56.779]Container killed on request. Exit code is 143\n",
      "[2022-03-18 23:47:56.780]Container exited with a non-zero exit code 143. \n",
      "[2022-03-18 23:47:56.784]Killed by external signal\n",
      ".\n",
      "22/03/18 23:47:56 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6155.0 in stage 8.0 (TID 40661) (hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647641937773_0001_01_000022 on host: hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-18 23:47:56.779]Container killed on request. Exit code is 143\n",
      "[2022-03-18 23:47:56.780]Container exited with a non-zero exit code 143. \n",
      "[2022-03-18 23:47:56.784]Killed by external signal\n",
      ".\n",
      "22/03/18 23:47:56 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6190.0 in stage 8.0 (TID 40696) (hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647641937773_0001_01_000022 on host: hub-msca-bdp-dphub-students-bhadri-sw-50zn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-18 23:47:56.779]Container killed on request. Exit code is 143\n",
      "[2022-03-18 23:47:56.780]Container exited with a non-zero exit code 143. \n",
      "[2022-03-18 23:47:56.784]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "covid_df.write.partitionBy(\"created_at_year\", \"created_at_mon\", \"created_at_day\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"gs://msca-bdp-students-bucket/shared_data/bhadri/covid_tweets4_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Caught CTRL-C (signal 2) - exiting\n"
     ]
    }
   ],
   "source": [
    "!gsutil du -sh -a gs://msca-bdp-students-bucket/shared_data/bhadri/covid_tweets_df.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 00:12:29 WARN org.apache.spark.sql.execution.datasources.SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "22/03/18 00:12:35 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "#Read\n",
    "\n",
    "covid_tweets_df = spark.read.parquet(\"gs://msca-bdp-students-bucket/shared_data/bhadri/covid_tweets_df.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617.8 G  617.8 G  gs://msca-bdp-tweets/final_project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = 'hadoop fs -du -s -h ' + directory\n",
    "\n",
    "p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "for line in p.stdout.readlines():\n",
    "    print (line)\n",
    "    \n",
    "retval = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 335 ms, total: 1.46 s\n",
      "Wall time: 4min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100011000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tweets_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_write = 'msca-bdp-students-bucket/shared_data/bhadri/tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file1_path = 'gs://msca-bdp-students-bucket/shared_data/bhadri/tweets50k.csv'\n",
    "#load csv to df\n",
    "twr50k_df = spark.read.csv(file1_path, header='true', inferSchema='true', sep=',', quote='\"')\n",
    "# twr50k_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>contributors</th><th>coordinates</th><th>created_at</th><th>display_text_range</th><th>entities</th><th>extended_entities</th><th>extended_tweet</th><th>favorite_count</th><th>favorited</th><th>filter_level</th><th>geo</th><th>id</th><th>id_str</th><th>in_reply_to_screen_name</th><th>in_reply_to_status_id</th><th>in_reply_to_status_id_str</th><th>in_reply_to_user_id</th><th>in_reply_to_user_id_str</th><th>is_quote_status</th><th>lang</th><th>place</th><th>possibly_sensitive</th><th>quote_count</th><th>quoted_status</th><th>quoted_status_id</th><th>quoted_status_id_str</th><th>quoted_status_permalink</th><th>reply_count</th><th>retweet_count</th><th>retweeted</th><th>retweeted_status</th><th>source</th><th>text</th><th>timestamp_ms</th><th>truncated</th><th>user</th><th>withheld_in_countries</th></tr>\n",
       "<tr><td>0</td><td>null</td><td>null</td><td>Sun Jan 16 20:55:...</td><td>null</td><td>Row(hashtags=[], ...</td><td>null</td><td>null</td><td>0</td><td>False</td><td>low</td><td>null</td><td>1482818815730008064</td><td>1482818815730008064</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>False</td><td>en</td><td>null</td><td>False</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>0</td><td>False</td><td>&quot;Row(contributors...</td><td> text=&#x27;COVID 🦠 A...</td><td> truncated=False</td><td> user=Row(contrib...</td><td> created_at=&#x27;Thu ...</td><td> default_profile=...</td><td> default_profile_...</td></tr>\n",
       "<tr><td>1</td><td>null</td><td>null</td><td>Sun Jan 16 20:55:...</td><td>null</td><td>Row(hashtags=[], ...</td><td>null</td><td>&quot;Row(display_text...</td><td>0</td><td>False</td><td>low</td><td>null</td><td>1482818815960526850</td><td>1482818815960526850</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>False</td><td>en</td><td>null</td><td>False</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>0</td><td>False</td><td>null</td><td>&quot;&lt;a href=&quot;&quot;https:...</td><td>Maryland Gov. Hog...</td><td>1642366529823</td><td>True</td><td>Row(contributors_...</td><td>null</td></tr>\n",
       "<tr><td>2</td><td>null</td><td>null</td><td>Sun Jan 16 20:55:...</td><td>null</td><td>Row(hashtags=[], ...</td><td>null</td><td>&quot;Row(display_text...</td><td> at least in NJ. ...</td><td>000 at the peak to 2</td><td>400 yesterday.  \\...</td><td>0</td><td>False</td><td>low</td><td>null</td><td>1482818816027541510</td><td>1482818816027541510</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>True</td><td>en</td><td>null</td><td>null</td><td>0</td><td>&quot;Row(contributors...</td><td> text=&#x27;Do the Omi...</td><td> truncated=False</td><td> user=Row(contrib...</td><td> created_at=&#x27;Sat ...</td><td> default_profile=...</td><td> default_profile_...</td><td> description=&#x27;44 ...</td><td> Bears</td><td> Bulls</td><td> Black Hawks</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+------------+-----------+--------------------+------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
       "|_c0|contributors|coordinates|          created_at|display_text_range|            entities|extended_entities|      extended_tweet|      favorite_count|           favorited|        filter_level| geo|                 id|             id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|place|possibly_sensitive|quote_count|quoted_status|quoted_status_id|quoted_status_id_str|quoted_status_permalink|         reply_count|   retweet_count|           retweeted|    retweeted_status|              source|                text|        timestamp_ms|           truncated|                user|withheld_in_countries|\n",
       "+---+------------+-----------+--------------------+------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
       "|  0|        null|       null|Sun Jan 16 20:55:...|              null|Row(hashtags=[], ...|             null|                null|                   0|               False|                 low|null|1482818815730008064|1482818815730008064|                   null|                 null|                     null|               null|                   null|          False|  en| null|             False|          0|         null|            null|                null|                   null|                   0|               0|               False|\"Row(contributors...| text='COVID 🦠 A...|     truncated=False| user=Row(contrib...| created_at='Thu ...| default_profile=...|  default_profile_...|\n",
       "|  1|        null|       null|Sun Jan 16 20:55:...|              null|Row(hashtags=[], ...|             null|\"Row(display_text...|                   0|               False|                 low|null|1482818815960526850|1482818815960526850|                   null|                 null|                     null|               null|                   null|          False|  en| null|             False|          0|         null|            null|                null|                   null|                   0|               0|               False|                null|\"<a href=\"\"https:...|Maryland Gov. Hog...|       1642366529823|                True|Row(contributors_...|                 null|\n",
       "|  2|        null|       null|Sun Jan 16 20:55:...|              null|Row(hashtags=[], ...|             null|\"Row(display_text...| at least in NJ. ...|000 at the peak to 2|400 yesterday.  \\...|   0|              False|                low|                   null|  1482818816027541510|      1482818816027541510|               null|                   null|           null|null| null|              True|         en|         null|            null|                   0|   \"Row(contributors...| text='Do the Omi...| truncated=False| user=Row(contrib...| created_at='Sat ...| default_profile=...| default_profile_...| description='44 ...|               Bears|               Bulls|          Black Hawks|\n",
       "+---+------------+-----------+--------------------+------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+----+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+-----+------------------+-----------+-------------+----------------+--------------------+-----------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twr50k_df.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testdf = twr50k_df.filter(twr50k_df.retweeted_status.contains('covid')).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdf.to_csv('gs://msca-bdp-students-bucket/shared_data/bhadri/testdf2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|lang|\n",
      "+----+\n",
      "|  en|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sample_df.select('lang').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(_c0,StringType,true),StructField(contributors,StringType,true),StructField(coordinates,StringType,true),StructField(created_at,StringType,true),StructField(display_text_range,StringType,true),StructField(entities,StringType,true),StructField(extended_entities,StringType,true),StructField(extended_tweet,StringType,true),StructField(favorite_count,StringType,true),StructField(favorited,StringType,true),StructField(filter_level,StringType,true),StructField(geo,StringType,true),StructField(id,StringType,true),StructField(id_str,StringType,true),StructField(in_reply_to_screen_name,StringType,true),StructField(in_reply_to_status_id,StringType,true),StructField(in_reply_to_status_id_str,StringType,true),StructField(in_reply_to_user_id,StringType,true),StructField(in_reply_to_user_id_str,StringType,true),StructField(is_quote_status,StringType,true),StructField(lang,StringType,true),StructField(place,StringType,true),StructField(possibly_sensitive,StringType,true),StructField(quote_count,StringType,true),StructField(quoted_status,StringType,true),StructField(quoted_status_id,StringType,true),StructField(quoted_status_id_str,StringType,true),StructField(quoted_status_permalink,StringType,true),StructField(reply_count,StringType,true),StructField(retweet_count,StringType,true),StructField(retweeted,StringType,true),StructField(retweeted_status,StringType,true),StructField(source,StringType,true),StructField(text,StringType,true),StructField(timestamp_ms,StringType,true),StructField(truncated,StringType,true),StructField(user,StringType,true),StructField(withheld_in_countries,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(twr50k_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'display_text_range',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'extended_tweet',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'filter_level',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quote_count',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'quoted_status_permalink',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'timestamp_ms',\n",
       " 'truncated',\n",
       " 'user',\n",
       " 'withheld_in_countries']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twr50k_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#null in imp cols\n",
    "col_list = ['user','text']\n",
    "col_list2 = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| user| text|\n",
      "+-----+-----+\n",
      "|15088|13832|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "twr50k_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|user|text|\n",
      "+----+----+\n",
      "|   0|   0|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "covid_df.select([count(when(col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid_df = covid_df.withColumn(\"created_at_temp\", covid_df.created_at.substr(5,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "# covid_df = covid_df.withColumn(\"created_at_new\", to_date(\"created_at\",\"EEE MMM DD HH:mm:ss ZZ yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = covid_df.drop(\"created_at_new\",\"created_at_month\",\"created_at_dayofmonth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid_df = covid_df.withColumn(\"created_at_year\", year(covid_df.created_at_new))\n",
    "# covid_df = covid_df.withColumn(\"created_at_month\", month(covid_df.created_at_new))\n",
    "# covid_df = covid_df.withColumn(\"created_at_dayofmonth\", dayofmonth(covid_df.created_at_new))\n",
    "# covid_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|created_at|\n",
      "+----------+\n",
      "|         0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_list = ['created_at_year']\n",
    "covid_df.select([count(when(col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|created_at_temp|\n",
      "+---------------+\n",
      "|              0|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_list = ['created_at_temp']\n",
    "covid_df.select([count(when(col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'covid_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m col_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcovid_df\u001b[49m\u001b[38;5;241m.\u001b[39mselect([count(when(col(c)\u001b[38;5;241m.\u001b[39misNull(), c))\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m col_list])\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'covid_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>place</th><th>text</th><th>extended_tweet</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>created_at_year</th><th>created_at_mon</th><th>created_at_day</th></tr>\n",
       "<tr><td>1465067953163055105</td><td>1465067953163055105</td><td>{false, Fri Nov 1...</td><td>null</td><td>RT @GregAbbott_TX...</td><td>null</td><td>{[], null, [], []...</td><td>Sun Nov 28 21:19:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2021</td><td>11</td><td>28</td></tr>\n",
       "<tr><td>1465067953964163075</td><td>1465067953964163075</td><td>{false, Sat Jan 1...</td><td>null</td><td>RT @SunScotNation...</td><td>null</td><td>{[], null, [], [{...</td><td>Sun Nov 28 21:19:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2021</td><td>11</td><td>28</td></tr>\n",
       "<tr><td>1465067954027085831</td><td>1465067954027085831</td><td>{false, Mon Sep 1...</td><td>null</td><td>RT @Covid19Critic...</td><td>null</td><td>{[], null, [], []...</td><td>Sun Nov 28 21:19:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2021</td><td>11</td><td>28</td></tr>\n",
       "<tr><td>1465067954547007495</td><td>1465067954547007495</td><td>{false, Thu Dec 1...</td><td>null</td><td>RT @MeidasTouch: ...</td><td>null</td><td>{[], null, [], []...</td><td>Sun Nov 28 21:19:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>true</td><td>false</td><td>en</td><td>2021</td><td>11</td><td>28</td></tr>\n",
       "<tr><td>1465067954685554688</td><td>1465067954685554688</td><td>{false, Mon Jan 1...</td><td>null</td><td>RT @catturd2: The...</td><td>null</td><td>{[], null, [], []...</td><td>Sun Nov 28 21:19:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2021</td><td>11</td><td>28</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------+--------------+--------------+\n",
       "|                 id|             id_str|                user|place|                text|      extended_tweet|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|created_at_year|created_at_mon|created_at_day|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------+--------------+--------------+\n",
       "|1477503728857006080|1477503728857006080|{false, Sun Dec 0...| null|@math_rachel @mat...|                null|{[], null, [], []...|Sun Jan 02 04:55:...|       null|             0|          0|            0|          0|          false|    false|  en|           2022|             1|            02|\n",
       "|1477503728861138947|1477503728861138947|{false, Wed Sep 0...| null|@tanishkamascara ...|{[32, 187], {[], ...|{[], null, [], [{...|Sun Jan 02 04:55:...|       null|             0|          0|            0|          0|          false|    false|  en|           2022|             1|            02|\n",
       "|1477503729368645632|1477503729368645632|{false, Thu Jun 1...| null|People Of The Yea...|                null|{[], null, [], [{...|Sun Jan 02 04:55:...|       null|             0|          0|            0|          0|          false|    false|  en|           2022|             1|            02|\n",
       "|1477503729641443333|1477503729641443333|{false, Sun Oct 1...| null|RT @JennaEllisEsq...|                null|{[], null, [], []...|Sun Jan 02 04:55:...|       null|             0|          0|            0|          0|           true|    false|  en|           2022|             1|            02|\n",
       "|1477503729750331392|1477503729750331392|{false, Mon Oct 1...| null|RT @MSHUSTLE1271:...|                null|{[], null, [], []...|Sun Jan 02 04:55:...|       null|             0|          0|            0|          0|          false|    false|  en|           2022|             1|            02|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+---------------+--------------+--------------+"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 22:19:50 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 65 for reason Container marked as failed: container_1647546496475_0001_01_000066 on host: hub-msca-bdp-dphub-students-backup-bhadri-sw-9l8f.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "22/03/17 22:19:50 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 65 on hub-msca-bdp-dphub-students-backup-bhadri-sw-9l8f.c.msca-bdp-students.internal: Container marked as failed: container_1647546496475_0001_01_000066 on host: hub-msca-bdp-dphub-students-backup-bhadri-sw-9l8f.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>place</th><th>text</th><th>extended_tweet</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>created_at_new</th><th>created_at_year</th><th>created_at_month</th><th>created_at_dayofmonth</th><th>created_at_mon</th><th>created_at_day</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+------+----+-----+----+--------------+--------+----------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------+---------------+----------------+---------------------+--------------+--------------+\n",
       "| id|id_str|user|place|text|extended_tweet|entities|created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|created_at_new|created_at_year|created_at_month|created_at_dayofmonth|created_at_mon|created_at_day|\n",
       "+---+------+----+-----+----+--------------+--------+----------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------+---------------+----------------+---------------------+--------------+--------------+\n",
       "+---+------+----+-----+----+--------------+--------+----------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------+---------------+----------------+---------------------+--------------+--------------+"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.filter(covid_df.created_at_year.isNull()).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|   place|coordinates|\n",
      "+--------+-----------+\n",
      "|61430539|   61710934|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_list = ['place','coordinates']\n",
    "covid_tweets_df.select([count(when(col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = covid_df.filter(covid_df.created_at_year.isNull()).limit(10000)\n",
    "test_df.write.mode(\"overwrite\").parquet(\"gs://msca-bdp-students-bucket/shared_data/bhadri/test_date_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_date_pq_df = spark.read.parquet(\"gs://msca-bdp-students-bucket/shared_data/bhadri/test_date_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>place</th><th>text</th><th>extended_tweet</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th></tr>\n",
       "<tr><td>1467276260103630850</td><td>1467276260103630850</td><td>{false, Wed Jun 0...</td><td>null</td><td>RT @TheKaranMenon...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 04 23:34:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td></tr>\n",
       "<tr><td>1467276260736782338</td><td>1467276260736782338</td><td>{false, Wed Jun 0...</td><td>null</td><td>RT @WORLDMUSICAWA...</td><td>null</td><td>{[{[21, 37], YGEn...</td><td>Sat Dec 04 23:34:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td></tr>\n",
       "<tr><td>1467276260598554631</td><td>1467276260598554631</td><td>{false, Thu Oct 1...</td><td>null</td><td>RT @TroubledSamm:...</td><td>null</td><td>{[], null, [], []...</td><td>Sat Dec 04 23:34:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>true</td><td>false</td><td>en</td></tr>\n",
       "<tr><td>1467276260787515393</td><td>1467276260787515393</td><td>{false, Thu Jul 1...</td><td>null</td><td>RT @disclosetv: L...</td><td>null</td><td>{[{[128, 136], CO...</td><td>Sat Dec 04 23:34:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td></tr>\n",
       "<tr><td>1467276261525372929</td><td>1467276261525372929</td><td>{false, Tue Oct 0...</td><td>null</td><td>RT @MackayIM: &quot;Qu...</td><td>null</td><td>{[], null, [], [{...</td><td>Sat Dec 04 23:34:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+\n",
       "|                 id|             id_str|                user|place|                text|extended_tweet|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+\n",
       "|1467276260103630850|1467276260103630850|{false, Wed Jun 0...| null|RT @TheKaranMenon...|          null|{[], null, [], []...|Sat Dec 04 23:34:...|       null|             0|          0|            0|          0|          false|    false|  en|\n",
       "|1467276260736782338|1467276260736782338|{false, Wed Jun 0...| null|RT @WORLDMUSICAWA...|          null|{[{[21, 37], YGEn...|Sat Dec 04 23:34:...|       null|             0|          0|            0|          0|          false|    false|  en|\n",
       "|1467276260598554631|1467276260598554631|{false, Thu Oct 1...| null|RT @TroubledSamm:...|          null|{[], null, [], []...|Sat Dec 04 23:34:...|       null|             0|          0|            0|          0|           true|    false|  en|\n",
       "|1467276260787515393|1467276260787515393|{false, Thu Jul 1...| null|RT @disclosetv: L...|          null|{[{[128, 136], CO...|Sat Dec 04 23:34:...|       null|             0|          0|            0|          0|          false|    false|  en|\n",
       "|1467276261525372929|1467276261525372929|{false, Tue Oct 0...| null|RT @MackayIM: \"Qu...|          null|{[], null, [], [{...|Sat Dec 04 23:34:...|       null|             0|          0|            0|          0|          false|    false|  en|\n",
       "+-------------------+-------------------+--------------------+-----+--------------------+--------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_date_pq_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_pq_df = test_date_pq_df.drop(\"created_at_temp\",\"created_at_mon_R\",\"created_at_yr_R\",\"created_at_day_R\",\"created_at_mon_R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_new\", to_date(\"created_at\",\"EEE MMM dd HH:mm:ss ZZ yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_year\", year(test_date_pq_df.created_at_new))\n",
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_month\", month(test_date_pq_df.created_at_new))\n",
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_dayofmonth\", dayofmonth(test_date_pq_df.created_at_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_mon_R\", month(to_date(trim(regexp_extract(\"created_at\",\"\\s\\w+\",0)),\"MMM\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_yr_R\", regexp_extract(\"created_at\",\"20\\d\\d$\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_pq_df = test_date_pq_df.withColumn(\"created_at_day_R\", trim(regexp_extract(\"created_at\",\"\\s\\d\\d\\s\",0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 18:26:47 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1647536877841_0001_01_000050 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.240]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.241]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1647536877841_0001_01_000048 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.241]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.246]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 49 for reason Container from a bad node: container_1647536877841_0001_01_000050 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.240]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.241]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 47 for reason Container from a bad node: container_1647536877841_0001_01_000048 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.241]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.246]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 49 on hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal: Container from a bad node: container_1647536877841_0001_01_000050 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.240]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.241]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2435.0 in stage 33.0 (TID 54311) (hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal executor 49): ExecutorLostFailure (executor 49 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647536877841_0001_01_000050 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.240]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.241]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2419.0 in stage 33.0 (TID 54295) (hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal executor 49): ExecutorLostFailure (executor 49 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647536877841_0001_01_000050 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.240]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.241]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 47 on hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal: Container from a bad node: container_1647536877841_0001_01_000048 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.241]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.246]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2425.0 in stage 33.0 (TID 54301) (hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal executor 47): ExecutorLostFailure (executor 47 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647536877841_0001_01_000048 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.241]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.246]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2431.0 in stage 33.0 (TID 54307) (hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal executor 47): ExecutorLostFailure (executor 47 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647536877841_0001_01_000048 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:26:47.241]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:26:47.241]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:26:47.246]Killed by external signal\n",
      ".\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 51 for reason Container marked as failed: container_1647536877841_0001_01_000052 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "22/03/17 18:26:47 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 50 for reason Container marked as failed: container_1647536877841_0001_01_000051 on host: hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "22/03/17 18:27:39 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1647536877841_0001_01_000021 on host: hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:27:39.081]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:27:39.081]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:27:39.081]Killed by external signal\n",
      ".\n",
      "22/03/17 18:27:39 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 20 for reason Container from a bad node: container_1647536877841_0001_01_000021 on host: hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:27:39.081]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:27:39.081]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:27:39.081]Killed by external signal\n",
      ".\n",
      "22/03/17 18:27:39 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 20 on hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal: Container from a bad node: container_1647536877841_0001_01_000021 on host: hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:27:39.081]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:27:39.081]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:27:39.081]Killed by external signal\n",
      ".\n",
      "22/03/17 18:27:39 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2764.0 in stage 33.0 (TID 54644) (hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647536877841_0001_01_000021 on host: hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:27:39.081]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:27:39.081]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:27:39.081]Killed by external signal\n",
      ".\n",
      "22/03/17 18:27:39 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2735.0 in stage 33.0 (TID 54615) (hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Container from a bad node: container_1647536877841_0001_01_000021 on host: hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-03-17 18:27:39.081]Container killed on request. Exit code is 143\n",
      "[2022-03-17 18:27:39.081]Container exited with a non-zero exit code 143. \n",
      "[2022-03-17 18:27:39.081]Killed by external signal\n",
      ".\n",
      "22/03/17 18:34:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 35.0 (TID 60050) (hub-msca-bdp-dphub-students-bhadri-w-1.c.msca-bdp-students.internal executor 42): FetchFailed(BlockManagerId(49, hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal, 7337, None), shuffleId=8, mapIndex=6, mapId=51882, reduceId=42, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=75164446001,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1647536877841_0001, execId=49)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:185)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:168)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:412)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:378)\n",
      "\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n",
      "\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=75164446001,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1647536877841_0001, execId=49)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:185)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:168)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:412)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:378)\n",
      "\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n",
      "\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "22/03/17 18:34:39 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 35.1 (TID 60309) (hub-msca-bdp-dphub-students-bhadri-sw-j146.c.msca-bdp-students.internal executor 61): FetchFailed(BlockManagerId(47, hub-msca-bdp-dphub-students-bhadri-sw-rfxn.c.msca-bdp-students.internal, 7337, None), shuffleId=8, mapIndex=0, mapId=51876, reduceId=42, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=75164446004,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1647536877841_0001, execId=47)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:185)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:168)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:412)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:378)\n",
      "\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n",
      "\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=75164446004,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1647536877841_0001, execId=47)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:185)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:168)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:412)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:378)\n",
      "\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n",
      "\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "22/03/17 18:35:18 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 35.2 (TID 60576) (hub-msca-bdp-dphub-students-bhadri-sw-710z.c.msca-bdp-students.internal executor 59): FetchFailed(BlockManagerId(20, hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal, 7337, None), shuffleId=8, mapIndex=3, mapId=51879, reduceId=42, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal:7337\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:685)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal:7337\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:101)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: java.net.UnknownHostException: hub-msca-bdp-dphub-students-bhadri-sw-f95w.c.msca-bdp-students.internal\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1281)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat java.net.InetAddress.getByName(InetAddress.java:1077)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:984)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:504)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:417)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:474)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\t... 2 more\n",
      "\n",
      ")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>created_at_year</th><th>created_at_month</th><th>count(id)</th></tr>\n",
       "<tr><td>null</td><td>null</td><td>41163506</td></tr>\n",
       "<tr><td>2022</td><td>1</td><td>20555993</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------+----------------+---------+\n",
       "|created_at_year|created_at_month|count(id)|\n",
       "+---------------+----------------+---------+\n",
       "|           null|            null| 41163506|\n",
       "|           2022|               1| 20555993|\n",
       "+---------------+----------------+---------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.groupBy('created_at_year','created_at_month') \\\n",
    "        .agg(count(covid_df.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "covid_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "806681"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_df = sample_df.filter(sql_fun.lower(sample_df.text).contains('corona'))\n",
    "corona_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# corona_df.write.mode(\"overwrite\").parquet(\"gs://msca-bdp-students-bucket/shared_data/bhadri/corona_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338225028   gs://msca-bdp-students-bucket/shared_data/bhadri/corona_df.parquet\n"
     ]
    }
   ],
   "source": [
    "!gsutil du -sh -a gs://msca-bdp-students-bucket/shared_data/bhadri/corona_df.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Parquet Corona_pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_pq_df = spark.read.parquet(\"gs://msca-bdp-students-bucket/shared_data/bhadri/corona_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>text</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>created_at_new</th><th>year</th><th>month</th></tr>\n",
       "<tr><td>1481853331467567104</td><td>1481853331467567104</td><td>{false, Fri Dec 1...</td><td>When did we switc...</td><td>{[], null, [], [{...</td><td>Fri Jan 14 04:59:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2022-01-14</td><td>2022</td><td>1</td></tr>\n",
       "<tr><td>1481853332931203072</td><td>1481853332931203072</td><td>{false, Mon Sep 0...</td><td>RT @TruthGundlach...</td><td>{[], null, [], []...</td><td>Fri Jan 14 04:59:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2022-01-14</td><td>2022</td><td>1</td></tr>\n",
       "<tr><td>1481853344855511042</td><td>1481853344855511042</td><td>{false, Sat Jun 2...</td><td>RT @VigilantFox: ...</td><td>{[], null, [], []...</td><td>Fri Jan 14 04:59:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2022-01-14</td><td>2022</td><td>1</td></tr>\n",
       "<tr><td>1481853369740410882</td><td>1481853369740410882</td><td>{false, Mon Jul 2...</td><td>RT @InternetHippo...</td><td>{[], null, [], []...</td><td>Fri Jan 14 04:59:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>true</td><td>false</td><td>en</td><td>2022-01-14</td><td>2022</td><td>1</td></tr>\n",
       "<tr><td>1481853374861479939</td><td>1481853374861479939</td><td>{false, Sat Mar 2...</td><td>This!\n",
       "&quot;42% of eli...</td><td>{[{[71, 89], Indi...</td><td>Fri Jan 14 04:59:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>2022-01-14</td><td>2022</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------+----+-----+\n",
       "|                 id|             id_str|                user|                text|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|created_at_new|year|month|\n",
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------+----+-----+\n",
       "|1478517061122859009|1478517061122859009|{false, Tue Aug 1...|RT @MamalehTrumpO...|{[{[36, 56], Janu...|Wed Jan 05 00:01:...|       null|             0|          0|            0|          0|          false|    false|  en|    2022-01-05|2022|    1|\n",
       "|1478517083700879360|1478517083700879360|{false, Sun May 2...|Coronavirus Today...|{[], null, [], [{...|Wed Jan 05 00:01:...|       null|             0|          0|            0|          0|          false|    false|  en|    2022-01-05|2022|    1|\n",
       "|1478517104676675585|1478517104676675585|{false, Thu Sep 0...|Kids, they get mi...|{[], null, [], [{...|Wed Jan 05 00:02:...|       null|             0|          0|            0|          0|           true|    false|  en|    2022-01-05|2022|    1|\n",
       "|1479339518171832320|1479339518171832320|{false, Fri Jun 2...|125 passengers on...|{[{[18, 24], Ital...|Fri Jan 07 06:30:...|       null|             0|          0|            0|          0|          false|    false|  en|    2022-01-07|2022|    1|\n",
       "|1479339577663901698|1479339577663901698|{false, Mon Mar 2...|RT @MeghBulletin:...|{[{[53, 65], coro...|Fri Jan 07 06:30:...|       null|             0|          0|            0|          0|          false|    false|  en|    2022-01-07|2022|    1|\n",
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------+----+-----+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_pq_df = corona_pq_df.withColumn(\"created_at_year\", year(corona_pq_df.created_at_new))\n",
    "corona_pq_df = corona_pq_df.withColumn(\"created_at_month\", month(corona_pq_df.created_at_new))\n",
    "corona_pq_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year(created_at_new)</th><th>month(created_at_new)</th><th>count(id)</th></tr>\n",
       "<tr><td>null</td><td>null</td><td>579768</td></tr>\n",
       "<tr><td>2022</td><td>1</td><td>226913</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+---------+\n",
       "|year(created_at_new)|month(created_at_new)|count(id)|\n",
       "+--------------------+---------------------+---------+\n",
       "|                null|                 null|   579768|\n",
       "|                2022|                    1|   226913|\n",
       "+--------------------+---------------------+---------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_pq_df = corona_pq_df.withColumn(\"created_at_new\", to_date(corona_pq_df.created_at.substr(5,30),\"MMM DD HH:mm:ss xx yyyy\"))\n",
    "\n",
    "corona_pq_df.groupBy(year(\"created_at_new\"),month(\"created_at_new\")) \\\n",
    "        .agg(count(corona_pq_df.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "806681"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_pq_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>text</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>type</th></tr>\n",
       "<tr><td>1459541693749989386</td><td>1459541693749989386</td><td>{false, Sun Apr 2...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541695960436740</td><td>1459541695960436740</td><td>{false, Fri Aug 1...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541714323091458</td><td>1459541714323091458</td><td>{false, Mon Feb 0...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541720274804741</td><td>1459541720274804741</td><td>{false, Wed Nov 1...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541727757443073</td><td>1459541727757443073</td><td>{false, Thu Nov 0...</td><td>RT @iran_policy: ...</td><td>{[{[104, 109], Ir...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541750616309764</td><td>1459541750616309764</td><td>{false, Mon Jun 1...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541767619940357</td><td>1459541767619940357</td><td>{false, Sun Jan 0...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541767813013504</td><td>1459541767813013504</td><td>{false, Fri Nov 2...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:20:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541823995760642</td><td>1459541823995760642</td><td>{false, Sat May 1...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:21:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "<tr><td>1459541848171716609</td><td>1459541848171716609</td><td>{false, Wed Mar 1...</td><td>RT @DrEricDing: ?...</td><td>{[], null, [], []...</td><td>Sat Nov 13 15:21:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>tweet</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+-----+\n",
       "|                 id|             id_str|                user|                text|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang| type|\n",
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+-----+\n",
       "|1479856620087689217|1479856620087689217|{false, Thu Mar 1...|As the highly inf...|{[], null, [], [{...|Sat Jan 08 16:44:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856620771299330|1479856620771299330|{false, Sat Jan 0...|RT @lakshman0310:...|{[{[18, 35], UPSC...|Sat Jan 08 16:44:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856631840231425|1479856631840231425|{false, Mon Apr 0...|RT @SkyNews: COVI...|{[], null, [], []...|Sat Jan 08 16:44:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856676152877061|1479856676152877061|{false, Sun Dec 2...|RT @BiharHealthDe...|{[{[21, 39], Biha...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856685317378051|1479856685317378051|{false, Sun Sep 1...|RT @Aanchal_pib: ...|{[{[48, 66], Indi...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|           true|    false|  en|quote|\n",
       "|1479856692670119939|1479856692670119939|{false, Wed Sep 1...|RT @NELiveTV: #Na...|{[{[14, 23], Naga...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856697699180551|1479856697699180551|{false, Tue Aug 1...|Rt @wef \n",
       "COVID-19...|{[{[51, 63], coro...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856706398138370|1479856706398138370|{false, Thu Nov 0...|RT @JohnRoss43: \"...|{[], null, [], []...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "|1479856706624507907|1479856706624507907|{false, Tue Jul 2...|RT @BrettKelman: ...|{[], null, [], []...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|           true|    false|  en|quote|\n",
       "|1479856718293184521|1479856718293184521|{false, Sat Apr 1...|RT @reyesalaluf: ...|{[], null, [], []...|Sat Jan 08 16:45:...|       null|             0|          0|            0|          0|          false|    false|  en|tweet|\n",
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+-----+"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:========>                                          (1369 + 16) / 8168]\r"
     ]
    }
   ],
   "source": [
    "corona_pq_df.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "twr50k_df = twr50k_df.withColumn(\"created_at_new\", twr50k_df.created_at.substr(5,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "twr50k_df = twr50k_df.withColumn(\"created_at_new\", to_date(\"created_at_new\", \"MMM DD HH:mm:ss xx yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test1_pdf = covid_df.sample(0.001,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_pq_df = corona_pq_df.withColumn('user.name',col('user.name')) \\\n",
    "                .withColumn('user.scrname',col('user.screen_name')) \\\n",
    "                .withColumn('user.desc',col('user.description')) \\\n",
    "                .withColumn('user.id',col('user.id')) \\\n",
    "                .withColumn('user.loc',col('user.location')) \\\n",
    "                .withColumn('user.url',col('user.url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>id_str</th><th>user</th><th>text</th><th>entities</th><th>created_at</th><th>coordinates</th><th>favorite_count</th><th>quote_count</th><th>retweet_count</th><th>reply_count</th><th>is_quote_status</th><th>retweeted</th><th>lang</th><th>user.name</th><th>user.scrname</th><th>user.desc</th><th>user.id</th><th>user.loc</th><th>user.tzone</th></tr>\n",
       "<tr><td>1463574376809439234</td><td>1463574376809439234</td><td>{false, Sat May 0...</td><td>RT @MetroUK: Seve...</td><td>{[], null, [], []...</td><td>Wed Nov 24 18:24:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Keith Webster</td><td>Keiths4pennorth</td><td>General interest ...</td><td>1391101501188288517</td><td>Leeds, England</td><td>null</td></tr>\n",
       "<tr><td>1463574396195332097</td><td>1463574396195332097</td><td>{false, Wed Apr 2...</td><td>Coronavirus infec...</td><td>{[], null, [], [{...</td><td>Wed Nov 24 18:25:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Hindustan Times</td><td>htTweets</td><td>One of India&#x27;s la...</td><td>36327407</td><td>India</td><td>null</td></tr>\n",
       "<tr><td>1463574418962018306</td><td>1463574418962018306</td><td>{false, Fri Mar 1...</td><td>RT @htTweets: Cor...</td><td>{[], null, [], []...</td><td>Wed Nov 24 18:25:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Corona Update Bot</td><td>CoronaUpdateBot</td><td>@plutonicindia ma...</td><td>1238467680773894146</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>1463574434258821125</td><td>1463574434258821125</td><td>{false, Tue Mar 1...</td><td>RT @Tim_Roehn: &quot;I...</td><td>{[], null, [], []...</td><td>Wed Nov 24 18:25:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Hans 🟢</td><td>Darioooooo23</td><td>null</td><td>3093509801</td><td>Deutschland</td><td>null</td></tr>\n",
       "<tr><td>1463574442227994625</td><td>1463574442227994625</td><td>{false, Tue Mar 0...</td><td>RT @DailyMail: Aa...</td><td>{[], null, [], []...</td><td>Wed Nov 24 18:25:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>CovfefeForT</td><td>MeBeHealthy</td><td>Independent. Free...</td><td>516335684</td><td>Massachusetts, USA</td><td>null</td></tr>\n",
       "<tr><td>1463574490043006993</td><td>1463574490043006993</td><td>{false, Tue Apr 1...</td><td>RT @Zobo_mx: due ...</td><td>{[{[20, 28], COVI...</td><td>Wed Nov 24 18:25:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Morri Colson</td><td>ColsonMorri</td><td>God 🙏🏾 , Family...</td><td>983702386022764545</td><td>Ocilla, GA</td><td>null</td></tr>\n",
       "<tr><td>1463574608058241027</td><td>1463574608058241027</td><td>{false, Fri Dec 0...</td><td>RT @drafzalniaz3:...</td><td>{[], null, [], []...</td><td>Wed Nov 24 18:25:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Kropstar</td><td>Krop_star</td><td>null</td><td>939209455958482944</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>1463574647128174602</td><td>1463574647128174602</td><td>{false, Sat Oct 3...</td><td>RT @Aurea_MM: #CO...</td><td>{[{[14, 22], COVI...</td><td>Wed Nov 24 18:26:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Hugo Vásquez</td><td>hugovasquezv</td><td>Médico salubrista...</td><td>86438640</td><td>Barcelona, Cataluña</td><td>null</td></tr>\n",
       "<tr><td>1463574654250016773</td><td>1463574654250016773</td><td>{false, Sun Feb 1...</td><td>RT @ylenews: #Cor...</td><td>{[{[13, 25], Coro...</td><td>Wed Nov 24 18:26:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Metinee Buaphan🇹...</td><td>metinee_buaphan</td><td>The more you read...</td><td>2347099007</td><td>Tampere, Finland</td><td>null</td></tr>\n",
       "<tr><td>1464253476091871241</td><td>1464253476091871241</td><td>{false, Mon Sep 2...</td><td>RT @nichcarlson: ...</td><td>{[], null, [], []...</td><td>Fri Nov 26 15:23:...</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>false</td><td>false</td><td>en</td><td>Kate Duffy</td><td>kate__duffy</td><td>Junior Business R...</td><td>1176202838382403586</td><td>London, England</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------------+---------------+--------------------+-------------------+--------------------+----------+\n",
       "|                 id|             id_str|                user|                text|            entities|          created_at|coordinates|favorite_count|quote_count|retweet_count|reply_count|is_quote_status|retweeted|lang|           user.name|   user.scrname|           user.desc|            user.id|            user.loc|user.tzone|\n",
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------------+---------------+--------------------+-------------------+--------------------+----------+\n",
       "|1480994003986403333|1480994003986403333|{false, Tue Jun 1...|RT @beyond_proces...|{[], null, [], []...|Tue Jan 11 20:04:...|       null|             0|          0|            0|          0|          false|    false|  en|YoursINegritude (...|YoursINegritude|Moderna 2 time Bl...|           47529168|       United States|      null|\n",
       "|1480994013666852865|1480994013666852865|{false, Fri Dec 2...|Why are republica...|{[], null, [], []...|Tue Jan 11 20:04:...|       null|             0|          0|            0|          0|           true|    false|  en|      Charles Wright|Charles67909721|College Graduate,...|1474461424281919498|                null|      null|\n",
       "|1480994020503523329|1480994020503523329|{false, Tue Sep 0...|Canada will have ...|{[], null, [], [{...|Tue Jan 11 20:04:...|       null|             0|          0|            0|          0|          false|    false|  en|NorthBelle🇨🇦🇿?...|    NorthBelle4|CITOYENNE CANADIE...|1036816881347219457|Treaty 13 - The B...|      null|\n",
       "|1480994031102410752|1480994031102410752|{false, Sat Jan 2...|RT @nogg_the: @Ra...|{[], null, [], []...|Tue Jan 11 20:04:...|       null|             0|          0|            0|          0|          false|    false|  en|          Jolt'n Joe|        MoMan60|Produced R&R Conc...|          241580028|     California, USA|      null|\n",
       "|1480994083497816067|1480994083497816067|{false, Fri Mar 1...|RT @l_e_whyte: 'P...|{[], null, [], []...|Tue Jan 11 20:04:...|       null|             0|          0|            0|          0|          false|    false|  en|          Yvonneblue| islandlife2014|Intelligent, poli...|         2389738746|             Florida|      null|\n",
       "|1451256922200363008|1451256922200363008|{false, Thu Nov 1...|RT @CBS_Herridge:...|{[], null, [], []...|Thu Oct 21 18:39:...|       null|             0|          0|            0|          0|          false|    false|  en|🦞ANTONIOTTI⚔️ St...|     townerman1|Civil Rights Acti...|         4168430639|        Dystopia USA|      null|\n",
       "|1451256981587644417|1451256981587644417|{false, Tue Nov 2...|RT @jimsciutto: N...|{[], null, [], []...|Thu Oct 21 18:40:...|       null|             0|          0|            0|          0|          false|    false|  en|         Accept DOGE|     verycool21|         #AcceptDOGE|1067511351969492994|                null|      null|\n",
       "|1451256986130161673|1451256986130161673|{false, Wed Jun 2...|Coronavirus Pande...|{[], null, [], [{...|Thu Oct 21 18:40:...|       null|             0|          0|            0|          0|          false|    false|  en|  Williams Lake News|     wlLakenews|News aggregator f...|         3254193432|       Williams Lake|      null|\n",
       "|1451257000487227394|1451257000487227394|{false, Sun Jan 1...|RT @UKCovid19Stat...|{[], null, [], []...|Thu Oct 21 18:40:...|       null|             0|          0|            0|          0|          false|    false|  en|            wenywens|      wenywens1|Life is a journey...|         4820611235| Birmingham, England|      null|\n",
       "|1451257021261615114|1451257021261615114|{false, Mon Aug 2...|RT @CBS_Herridge:...|{[], null, [], []...|Thu Oct 21 18:40:...|       null|             0|          0|            0|          0|          false|    false|  en|Deplorable Carol ...|       granmanh|Mimi, loved Trump...|          364298687|frigid north NH a...|      null|\n",
       "+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+-----------+-------------+-----------+---------------+---------+----+--------------------+---------------+--------------------+-------------------+--------------------+----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_pq_df.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corona_pq_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m testdf \u001b[38;5;241m=\u001b[39m \u001b[43mcorona_pq_df\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(sql_fun\u001b[38;5;241m.\u001b[39mlower(corona_pq_df\u001b[38;5;241m.\u001b[39muser\u001b[38;5;241m.\u001b[39mdescription)\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corona_pq_df' is not defined"
     ]
    }
   ],
   "source": [
    "testdf = corona_pq_df.filter(sql_fun.lower(corona_pq_df.user.description).contains('student'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}